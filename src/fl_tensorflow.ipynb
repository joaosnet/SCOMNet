{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7332d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8b4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "screens = ['Focus', 'Mathisis', 'Memoria', 'Reacton', 'Speedy']\n",
    "screens_code = ['1', '2', '3', '4', '5']\n",
    "\n",
    "base_path = \"C:/Users/SouthSystem/Federated Learning/DataBioCom/data\"\n",
    "phone_accel_file_paths = []\n",
    "phone_gyro_file_paths = []\n",
    "\n",
    "for directories, subdirectories, files in os.walk(base_path):\n",
    "    for filename in files:\n",
    "        if \"accel\" in filename:\n",
    "            phone_accel_file_paths.append(f\"{base_path}/accel/{filename}\")\n",
    "            \n",
    "data = pd.concat(map(pd.read_csv, phone_accel_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344ec320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_accel</th>\n",
       "      <th>y_accel</th>\n",
       "      <th>z_accel</th>\n",
       "      <th>screen</th>\n",
       "      <th>player_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.146046</td>\n",
       "      <td>0.802520</td>\n",
       "      <td>0.586626</td>\n",
       "      <td>MemoriaGame - 1.1.1</td>\n",
       "      <td>06mdn3c</td>\n",
       "      <td>1536502712738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.031261</td>\n",
       "      <td>0.766375</td>\n",
       "      <td>0.666243</td>\n",
       "      <td>MemoriaGame - 1.1.1</td>\n",
       "      <td>06mdn3c</td>\n",
       "      <td>1536502712738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.042495</td>\n",
       "      <td>0.769794</td>\n",
       "      <td>0.647682</td>\n",
       "      <td>MemoriaGame - 1.1.1</td>\n",
       "      <td>06mdn3c</td>\n",
       "      <td>1536502712738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025888</td>\n",
       "      <td>0.766375</td>\n",
       "      <td>0.647194</td>\n",
       "      <td>MemoriaGame - 1.1.1</td>\n",
       "      <td>06mdn3c</td>\n",
       "      <td>1536502712738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028818</td>\n",
       "      <td>0.760025</td>\n",
       "      <td>0.643286</td>\n",
       "      <td>MemoriaGame - 1.1.1</td>\n",
       "      <td>06mdn3c</td>\n",
       "      <td>1536502712738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466372</th>\n",
       "      <td>-0.038849</td>\n",
       "      <td>-0.608170</td>\n",
       "      <td>-0.793686</td>\n",
       "      <td>FocusGame - 4.1.1</td>\n",
       "      <td>x8rbf3x</td>\n",
       "      <td>1547393235756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466373</th>\n",
       "      <td>-0.042145</td>\n",
       "      <td>-0.613846</td>\n",
       "      <td>-0.786880</td>\n",
       "      <td>FocusGame - 4.1.1</td>\n",
       "      <td>x8rbf3x</td>\n",
       "      <td>1547393235756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466374</th>\n",
       "      <td>-0.043121</td>\n",
       "      <td>-0.613129</td>\n",
       "      <td>-0.786621</td>\n",
       "      <td>FocusGame - 4.1.1</td>\n",
       "      <td>x8rbf3x</td>\n",
       "      <td>1547393235756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466375</th>\n",
       "      <td>-0.042694</td>\n",
       "      <td>-0.608475</td>\n",
       "      <td>-0.789032</td>\n",
       "      <td>FocusGame - 4.1.1</td>\n",
       "      <td>x8rbf3x</td>\n",
       "      <td>1547393235756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466376</th>\n",
       "      <td>-0.041931</td>\n",
       "      <td>-0.606552</td>\n",
       "      <td>-0.784882</td>\n",
       "      <td>FocusGame - 4.1.1</td>\n",
       "      <td>x8rbf3x</td>\n",
       "      <td>1547393235756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4014096 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x_accel   y_accel   z_accel               screen player_id  \\\n",
       "0      -0.146046  0.802520  0.586626  MemoriaGame - 1.1.1   06mdn3c   \n",
       "1      -0.031261  0.766375  0.666243  MemoriaGame - 1.1.1   06mdn3c   \n",
       "2      -0.042495  0.769794  0.647682  MemoriaGame - 1.1.1   06mdn3c   \n",
       "3      -0.025888  0.766375  0.647194  MemoriaGame - 1.1.1   06mdn3c   \n",
       "4      -0.028818  0.760025  0.643286  MemoriaGame - 1.1.1   06mdn3c   \n",
       "...          ...       ...       ...                  ...       ...   \n",
       "466372 -0.038849 -0.608170 -0.793686    FocusGame - 4.1.1   x8rbf3x   \n",
       "466373 -0.042145 -0.613846 -0.786880    FocusGame - 4.1.1   x8rbf3x   \n",
       "466374 -0.043121 -0.613129 -0.786621    FocusGame - 4.1.1   x8rbf3x   \n",
       "466375 -0.042694 -0.608475 -0.789032    FocusGame - 4.1.1   x8rbf3x   \n",
       "466376 -0.041931 -0.606552 -0.784882    FocusGame - 4.1.1   x8rbf3x   \n",
       "\n",
       "            timestamp  \n",
       "0       1536502712738  \n",
       "1       1536502712738  \n",
       "2       1536502712738  \n",
       "3       1536502712738  \n",
       "4       1536502712738  \n",
       "...               ...  \n",
       "466372  1547393235756  \n",
       "466373  1547393235756  \n",
       "466374  1547393235756  \n",
       "466375  1547393235756  \n",
       "466376  1547393235756  \n",
       "\n",
       "[4014096 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc42032",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = data['player_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2428133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(df):\n",
    "    array = df.values    \n",
    "    # array = df\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures - 1\n",
    "    X = array[:, 0:nfeatures]\n",
    "    y = array[:, -1]\n",
    "    \n",
    "    rows, cols = X.shape\n",
    "    \n",
    "    for i in range(0, rows):\n",
    "        row = X[i,:]\n",
    "        mu = np.mean( row )\n",
    "        sigma = np.std( row )\n",
    "        if( sigma == 0 ):\n",
    "            sigma = 0.0001\n",
    "        X[i,:] = (X[i,:] - mu) / sigma\n",
    "            \n",
    "    df = pd.DataFrame( X )\n",
    "    df['user'] = y \n",
    "    return df\n",
    "\n",
    "def unique(list1):       \n",
    "    list_set = set(list1) \n",
    "    unique_list = (list(list_set)) \n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "\n",
    "def create_userids( df ):\n",
    "    array = df.values\n",
    "    y = array[:, -1]\n",
    "    return unique( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf71bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_1():\n",
    "    user_list = []\n",
    "    frame_size = 500\n",
    "    step = 50\n",
    "    train = np.empty((0, frame_size, 3))\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        data_user = data_user[500:-500]\n",
    "        data_user = data_user.values\n",
    "        data_user = data_user.astype('float32')\n",
    "        frames = [data_user[i:i+frame_size, :] for i in range(0,data_user.shape[0]-frame_size,step)]\n",
    "        user_list.extend([user]*len(frames))           \n",
    "        frames = np.dstack(frames)\n",
    "        frames = np.rollaxis(frames,-1)\n",
    "        train = np.vstack((train, frames))\n",
    "        \n",
    "\n",
    "    return train, user_list\n",
    "\n",
    "def load_data_2():\n",
    "    user_list = []\n",
    "    train = []\n",
    "    frame_size = 128\n",
    "    step = 50\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        for w in range(0, data_user.shape[0] - frame_size, step):\n",
    "            end = w + frame_size        \n",
    "            frame = data_user.iloc[w:end,[0, 1, 2]]        \n",
    "            train.append(frame)\n",
    "            user_list.append(user)\n",
    "\n",
    "    return train, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3d29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_3():\n",
    "    data['session'] = data['player_id'] + \"_\" + data['timestamp'].apply(str)\n",
    "    \n",
    "    counts = data['session'].value_counts()\n",
    "    counts = counts[counts >= 1]\n",
    "    counts_list = list(counts.keys())\n",
    "    df = data[data.session.isin(counts_list) == True]\n",
    "    \n",
    "    for idx, val in enumerate(screens):\n",
    "        df.loc[df.screen.str.contains(screens[idx]), 'screen'] = screens_code[idx]\n",
    "        \n",
    "    win_count = 0\n",
    "    total_win_count = 0\n",
    "    range_screen = range(1, 6)\n",
    "    raw_signal = df\n",
    "    axis_list = ['x_accel', 'y_accel', 'z_accel']\n",
    "    user_list = []\n",
    "    window_size = 128\n",
    "    axis_dict = {}\n",
    "\n",
    "    for axis in axis_list:  \n",
    "        features_one = []\n",
    "        for class_label in range_screen:   \n",
    "            screen_ID = screens_code[class_label - 1]    \n",
    "            raw_data_one_activity = np.array(raw_signal.loc[raw_signal['screen'] == screen_ID, [axis]])\n",
    "            raw_data_one_activity = pd.DataFrame(raw_data_one_activity)   \n",
    "            player_id_data = np.array(raw_signal.loc[raw_signal['screen'] == screen_ID, ['player_id']])\n",
    "            player_id_data = pd.DataFrame(player_id_data)  \n",
    "\n",
    "            for data_point in range(0, len(raw_data_one_activity), window_size):        \n",
    "                win_count += 1\n",
    "                start = data_point\n",
    "                end = start + window_size\n",
    "                time_domain_window = raw_data_one_activity[start:end] \n",
    "\n",
    "                if (len(time_domain_window) == 128):                \n",
    "                    features_one.append(time_domain_window)\n",
    "                    if (axis == 'z_accel'):                    \n",
    "                        user_list.append(player_id_data[start:end][0].unique()[0])                    \n",
    "\n",
    "        axis_dict[axis] = features_one\n",
    "        \n",
    "    new = (axis_dict[axis_list[0]], axis_dict[axis_list[1]], axis_dict[axis_list[2]])\n",
    "    \n",
    "    new_x = new[0]    \n",
    "    new_x = np.array([np.array(x) for x in new_x])\n",
    "    new_x = new_x.reshape(new_x.shape[0],-1)\n",
    "    print(new_x.shape)\n",
    "    \n",
    "    new_y = new[1]\n",
    "    new_y = np.array([np.array(x) for x in new_y])  \n",
    "    new_y = new_y.reshape(new_y.shape[0],-1)\n",
    "    print(new_y.shape)\n",
    "\n",
    "    new_z = new[2]\n",
    "    new_z = np.array([np.array(x) for x in new_z])  \n",
    "    new_z = new_z.reshape(new_z.shape[0],-1)\n",
    "    print(new_z.shape)\n",
    "    \n",
    "    data_join = pd.DataFrame(np.concatenate((new_x, new_y, new_z), axis=1))\n",
    "    data_join['user'] = user_list\n",
    "    \n",
    "    return data_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1779cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31357, 128)\n",
      "(31357, 128)\n",
      "(31357, 128)\n"
     ]
    }
   ],
   "source": [
    "train_set_3 = load_data_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907b3c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006350</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>-0.004396</td>\n",
       "      <td>-0.010257</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>-0.008304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867484</td>\n",
       "      <td>0.850877</td>\n",
       "      <td>0.835735</td>\n",
       "      <td>0.837200</td>\n",
       "      <td>0.832315</td>\n",
       "      <td>0.830362</td>\n",
       "      <td>0.609095</td>\n",
       "      <td>0.806428</td>\n",
       "      <td>0.832804</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.049822</td>\n",
       "      <td>-0.054706</td>\n",
       "      <td>-0.033703</td>\n",
       "      <td>-0.025888</td>\n",
       "      <td>-0.017096</td>\n",
       "      <td>-0.021492</td>\n",
       "      <td>-0.046891</td>\n",
       "      <td>-0.011234</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.109412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773702</td>\n",
       "      <td>0.799589</td>\n",
       "      <td>0.766375</td>\n",
       "      <td>0.779563</td>\n",
       "      <td>0.774679</td>\n",
       "      <td>0.788355</td>\n",
       "      <td>0.816197</td>\n",
       "      <td>0.907537</td>\n",
       "      <td>0.810824</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038587</td>\n",
       "      <td>-0.091828</td>\n",
       "      <td>-0.264739</td>\n",
       "      <td>-0.301861</td>\n",
       "      <td>-0.128462</td>\n",
       "      <td>-0.054218</td>\n",
       "      <td>-0.057637</td>\n",
       "      <td>-0.064475</td>\n",
       "      <td>-0.074244</td>\n",
       "      <td>-0.046891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769794</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.785913</td>\n",
       "      <td>0.818639</td>\n",
       "      <td>0.750256</td>\n",
       "      <td>0.798124</td>\n",
       "      <td>0.771748</td>\n",
       "      <td>0.768817</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.046403</td>\n",
       "      <td>-0.046403</td>\n",
       "      <td>-0.070825</td>\n",
       "      <td>-0.070825</td>\n",
       "      <td>-0.060079</td>\n",
       "      <td>-0.056172</td>\n",
       "      <td>-0.040053</td>\n",
       "      <td>-0.050799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793728</td>\n",
       "      <td>0.818150</td>\n",
       "      <td>0.811801</td>\n",
       "      <td>0.809358</td>\n",
       "      <td>0.818639</td>\n",
       "      <td>0.823035</td>\n",
       "      <td>0.820104</td>\n",
       "      <td>0.826454</td>\n",
       "      <td>0.790309</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.010257</td>\n",
       "      <td>-0.060568</td>\n",
       "      <td>-0.039564</td>\n",
       "      <td>-0.033703</td>\n",
       "      <td>-0.025888</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>-0.031261</td>\n",
       "      <td>-0.031261</td>\n",
       "      <td>-0.047379</td>\n",
       "      <td>-0.040053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716553</td>\n",
       "      <td>0.868949</td>\n",
       "      <td>0.783471</td>\n",
       "      <td>0.837688</td>\n",
       "      <td>0.769306</td>\n",
       "      <td>0.782005</td>\n",
       "      <td>0.767352</td>\n",
       "      <td>0.758560</td>\n",
       "      <td>0.781028</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31352</th>\n",
       "      <td>-0.087982</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>-0.054306</td>\n",
       "      <td>-0.029510</td>\n",
       "      <td>-0.032318</td>\n",
       "      <td>-0.021622</td>\n",
       "      <td>-0.025528</td>\n",
       "      <td>-0.040161</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>-0.015244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713440</td>\n",
       "      <td>-0.397873</td>\n",
       "      <td>-0.779297</td>\n",
       "      <td>-0.775192</td>\n",
       "      <td>-0.621613</td>\n",
       "      <td>-0.775940</td>\n",
       "      <td>-0.740738</td>\n",
       "      <td>-0.732880</td>\n",
       "      <td>-0.670929</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31353</th>\n",
       "      <td>-0.212509</td>\n",
       "      <td>-0.056503</td>\n",
       "      <td>-0.258072</td>\n",
       "      <td>-0.054901</td>\n",
       "      <td>-0.095520</td>\n",
       "      <td>-0.128067</td>\n",
       "      <td>-0.076050</td>\n",
       "      <td>-0.077179</td>\n",
       "      <td>-0.097610</td>\n",
       "      <td>-0.086761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.745773</td>\n",
       "      <td>-0.706970</td>\n",
       "      <td>-0.692856</td>\n",
       "      <td>-0.694855</td>\n",
       "      <td>-0.715042</td>\n",
       "      <td>-0.856445</td>\n",
       "      <td>-0.751465</td>\n",
       "      <td>-0.854660</td>\n",
       "      <td>-0.361664</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31354</th>\n",
       "      <td>0.150406</td>\n",
       "      <td>0.028015</td>\n",
       "      <td>-0.100739</td>\n",
       "      <td>-0.136841</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>-0.070007</td>\n",
       "      <td>-0.162521</td>\n",
       "      <td>-0.069672</td>\n",
       "      <td>-0.120972</td>\n",
       "      <td>-0.166962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.845154</td>\n",
       "      <td>-1.011536</td>\n",
       "      <td>-0.916336</td>\n",
       "      <td>-0.913208</td>\n",
       "      <td>-1.038147</td>\n",
       "      <td>-0.893433</td>\n",
       "      <td>-1.014160</td>\n",
       "      <td>-0.892410</td>\n",
       "      <td>-0.915115</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31355</th>\n",
       "      <td>0.007492</td>\n",
       "      <td>-0.118988</td>\n",
       "      <td>-0.055145</td>\n",
       "      <td>-0.044754</td>\n",
       "      <td>-0.060135</td>\n",
       "      <td>-0.092682</td>\n",
       "      <td>-0.101700</td>\n",
       "      <td>-0.089127</td>\n",
       "      <td>-0.084930</td>\n",
       "      <td>-0.091431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890671</td>\n",
       "      <td>-0.897583</td>\n",
       "      <td>-1.032913</td>\n",
       "      <td>-0.385971</td>\n",
       "      <td>-0.926208</td>\n",
       "      <td>-0.726227</td>\n",
       "      <td>-0.895981</td>\n",
       "      <td>-0.910370</td>\n",
       "      <td>-0.772766</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31356</th>\n",
       "      <td>-0.088150</td>\n",
       "      <td>-0.093140</td>\n",
       "      <td>-0.087616</td>\n",
       "      <td>-0.076523</td>\n",
       "      <td>-0.080887</td>\n",
       "      <td>-0.088791</td>\n",
       "      <td>-0.015503</td>\n",
       "      <td>-0.089539</td>\n",
       "      <td>-0.059052</td>\n",
       "      <td>-0.046677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.974899</td>\n",
       "      <td>-0.859634</td>\n",
       "      <td>-0.828049</td>\n",
       "      <td>-1.111206</td>\n",
       "      <td>-0.895645</td>\n",
       "      <td>-0.989044</td>\n",
       "      <td>-0.928940</td>\n",
       "      <td>-0.870697</td>\n",
       "      <td>-1.349503</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31357 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.006350  0.001954  0.002442  0.001465 -0.004396 -0.010257 -0.001954   \n",
       "1     -0.049822 -0.054706 -0.033703 -0.025888 -0.017096 -0.021492 -0.046891   \n",
       "2      0.038587 -0.091828 -0.264739 -0.301861 -0.128462 -0.054218 -0.057637   \n",
       "3     -0.041030 -0.041030 -0.046403 -0.046403 -0.070825 -0.070825 -0.060079   \n",
       "4     -0.010257 -0.060568 -0.039564 -0.033703 -0.025888 -0.022957 -0.031261   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "31352 -0.087982  0.024231 -0.054306 -0.029510 -0.032318 -0.021622 -0.025528   \n",
       "31353 -0.212509 -0.056503 -0.258072 -0.054901 -0.095520 -0.128067 -0.076050   \n",
       "31354  0.150406  0.028015 -0.100739 -0.136841 -0.008072 -0.070007 -0.162521   \n",
       "31355  0.007492 -0.118988 -0.055145 -0.044754 -0.060135 -0.092682 -0.101700   \n",
       "31356 -0.088150 -0.093140 -0.087616 -0.076523 -0.080887 -0.088791 -0.015503   \n",
       "\n",
       "              7         8         9  ...       375       376       377  \\\n",
       "0      0.008792  0.006350 -0.008304  ...  0.867484  0.850877  0.835735   \n",
       "1     -0.011234  0.004396  0.109412  ...  0.773702  0.799589  0.766375   \n",
       "2     -0.064475 -0.074244 -0.046891  ...  0.769794  0.784936  0.784936   \n",
       "3     -0.056172 -0.040053 -0.050799  ...  0.793728  0.818150  0.811801   \n",
       "4     -0.031261 -0.047379 -0.040053  ...  0.716553  0.868949  0.783471   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "31352 -0.040161 -0.008102 -0.015244  ... -0.713440 -0.397873 -0.779297   \n",
       "31353 -0.077179 -0.097610 -0.086761  ... -0.745773 -0.706970 -0.692856   \n",
       "31354 -0.069672 -0.120972 -0.166962  ... -0.845154 -1.011536 -0.916336   \n",
       "31355 -0.089127 -0.084930 -0.091431  ... -0.890671 -0.897583 -1.032913   \n",
       "31356 -0.089539 -0.059052 -0.046677  ... -0.974899 -0.859634 -0.828049   \n",
       "\n",
       "            378       379       380       381       382       383     user  \n",
       "0      0.837200  0.832315  0.830362  0.609095  0.806428  0.832804  06mdn3c  \n",
       "1      0.779563  0.774679  0.788355  0.816197  0.907537  0.810824  06mdn3c  \n",
       "2      0.785913  0.818639  0.750256  0.798124  0.771748  0.768817  06mdn3c  \n",
       "3      0.809358  0.818639  0.823035  0.820104  0.826454  0.790309  06mdn3c  \n",
       "4      0.837688  0.769306  0.782005  0.767352  0.758560  0.781028  06mdn3c  \n",
       "...         ...       ...       ...       ...       ...       ...      ...  \n",
       "31352 -0.775192 -0.621613 -0.775940 -0.740738 -0.732880 -0.670929  x8rbf3x  \n",
       "31353 -0.694855 -0.715042 -0.856445 -0.751465 -0.854660 -0.361664  x8rbf3x  \n",
       "31354 -0.913208 -1.038147 -0.893433 -1.014160 -0.892410 -0.915115  x8rbf3x  \n",
       "31355 -0.385971 -0.926208 -0.726227 -0.895981 -0.910370 -0.772766  x8rbf3x  \n",
       "31356 -1.111206 -0.895645 -0.989044 -0.928940 -0.870697 -1.349503  x8rbf3x  \n",
       "\n",
       "[31357 rows x 385 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8befb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79395, 1501)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_1, user_list1 = load_data_1()\n",
    "train_set_join_1 = train_set_1.reshape(train_set_1.shape[0], 1500)\n",
    "data_join = pd.DataFrame(train_set_join_1)\n",
    "data_join['user'] = user_list1\n",
    "data_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff53cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2, user_list2 = load_data_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "915477ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2 = np.array([np.array(x) for x in train_set_2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31af7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_join_2 = train_set_2.reshape(train_set_2.shape[0], 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d124f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80220, 384)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_join_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb49954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80220, 385)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_join = pd.DataFrame(train_set_join_2)\n",
    "data_join['user'] = user_list2\n",
    "data_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073a5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f968399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.146046</td>\n",
       "      <td>0.802520</td>\n",
       "      <td>0.586626</td>\n",
       "      <td>-0.031261</td>\n",
       "      <td>0.766375</td>\n",
       "      <td>0.666243</td>\n",
       "      <td>-0.042495</td>\n",
       "      <td>0.769794</td>\n",
       "      <td>0.647682</td>\n",
       "      <td>-0.025888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>0.760025</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>-0.006350</td>\n",
       "      <td>0.763444</td>\n",
       "      <td>0.663312</td>\n",
       "      <td>-0.015142</td>\n",
       "      <td>0.773213</td>\n",
       "      <td>0.661847</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.771259</td>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.811312</td>\n",
       "      <td>0.539247</td>\n",
       "      <td>-0.040541</td>\n",
       "      <td>0.866995</td>\n",
       "      <td>0.520686</td>\n",
       "      <td>-0.059102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042983</td>\n",
       "      <td>0.786890</td>\n",
       "      <td>0.623260</td>\n",
       "      <td>-0.004884</td>\n",
       "      <td>0.777609</td>\n",
       "      <td>0.630098</td>\n",
       "      <td>-0.108435</td>\n",
       "      <td>0.807893</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023446</td>\n",
       "      <td>0.778098</td>\n",
       "      <td>0.655009</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.768329</td>\n",
       "      <td>0.650613</td>\n",
       "      <td>0.011723</td>\n",
       "      <td>0.773702</td>\n",
       "      <td>0.648659</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.621306</td>\n",
       "      <td>0.786401</td>\n",
       "      <td>0.014653</td>\n",
       "      <td>0.626679</td>\n",
       "      <td>0.783959</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.619841</td>\n",
       "      <td>0.780052</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003419</td>\n",
       "      <td>0.760025</td>\n",
       "      <td>0.643775</td>\n",
       "      <td>-0.005861</td>\n",
       "      <td>0.762956</td>\n",
       "      <td>0.662336</td>\n",
       "      <td>-0.003908</td>\n",
       "      <td>0.759537</td>\n",
       "      <td>0.664778</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042495</td>\n",
       "      <td>0.703365</td>\n",
       "      <td>0.703365</td>\n",
       "      <td>0.057637</td>\n",
       "      <td>0.652078</td>\n",
       "      <td>0.743418</td>\n",
       "      <td>0.050310</td>\n",
       "      <td>0.635471</td>\n",
       "      <td>0.778586</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033214</td>\n",
       "      <td>0.625702</td>\n",
       "      <td>0.784448</td>\n",
       "      <td>0.035168</td>\n",
       "      <td>0.613491</td>\n",
       "      <td>0.789820</td>\n",
       "      <td>0.040053</td>\n",
       "      <td>0.615445</td>\n",
       "      <td>0.783959</td>\n",
       "      <td>0.034680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022469</td>\n",
       "      <td>0.681874</td>\n",
       "      <td>0.725345</td>\n",
       "      <td>0.020026</td>\n",
       "      <td>0.682362</td>\n",
       "      <td>0.750256</td>\n",
       "      <td>0.020026</td>\n",
       "      <td>0.675524</td>\n",
       "      <td>0.740976</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80215</th>\n",
       "      <td>-0.043182</td>\n",
       "      <td>-0.609863</td>\n",
       "      <td>-0.767822</td>\n",
       "      <td>-0.044266</td>\n",
       "      <td>-0.609787</td>\n",
       "      <td>-0.778061</td>\n",
       "      <td>-0.041946</td>\n",
       "      <td>-0.607758</td>\n",
       "      <td>-0.787079</td>\n",
       "      <td>-0.039627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043823</td>\n",
       "      <td>-0.607330</td>\n",
       "      <td>-0.780472</td>\n",
       "      <td>-0.045563</td>\n",
       "      <td>-0.607315</td>\n",
       "      <td>-0.783386</td>\n",
       "      <td>-0.040222</td>\n",
       "      <td>-0.604111</td>\n",
       "      <td>-0.783401</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80216</th>\n",
       "      <td>-0.040771</td>\n",
       "      <td>-0.604309</td>\n",
       "      <td>-0.789261</td>\n",
       "      <td>-0.042007</td>\n",
       "      <td>-0.603592</td>\n",
       "      <td>-0.789734</td>\n",
       "      <td>-0.040344</td>\n",
       "      <td>-0.602325</td>\n",
       "      <td>-0.793396</td>\n",
       "      <td>-0.039627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036362</td>\n",
       "      <td>-0.609390</td>\n",
       "      <td>-0.790054</td>\n",
       "      <td>-0.037643</td>\n",
       "      <td>-0.607178</td>\n",
       "      <td>-0.793930</td>\n",
       "      <td>-0.040359</td>\n",
       "      <td>-0.603043</td>\n",
       "      <td>-0.794861</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80217</th>\n",
       "      <td>-0.039551</td>\n",
       "      <td>-0.605042</td>\n",
       "      <td>-0.789276</td>\n",
       "      <td>-0.039154</td>\n",
       "      <td>-0.607315</td>\n",
       "      <td>-0.777832</td>\n",
       "      <td>-0.040588</td>\n",
       "      <td>-0.605408</td>\n",
       "      <td>-0.774155</td>\n",
       "      <td>-0.043030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050720</td>\n",
       "      <td>-0.609039</td>\n",
       "      <td>-0.787018</td>\n",
       "      <td>-0.053665</td>\n",
       "      <td>-0.606857</td>\n",
       "      <td>-0.786743</td>\n",
       "      <td>-0.051178</td>\n",
       "      <td>-0.609070</td>\n",
       "      <td>-0.783844</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80218</th>\n",
       "      <td>-0.047516</td>\n",
       "      <td>-0.608307</td>\n",
       "      <td>-0.784119</td>\n",
       "      <td>-0.048035</td>\n",
       "      <td>-0.610962</td>\n",
       "      <td>-0.787308</td>\n",
       "      <td>-0.047577</td>\n",
       "      <td>-0.609482</td>\n",
       "      <td>-0.789734</td>\n",
       "      <td>-0.045151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041763</td>\n",
       "      <td>-0.605042</td>\n",
       "      <td>-0.790970</td>\n",
       "      <td>-0.043243</td>\n",
       "      <td>-0.605270</td>\n",
       "      <td>-0.792664</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>-0.604340</td>\n",
       "      <td>-0.789459</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80219</th>\n",
       "      <td>-0.051819</td>\n",
       "      <td>-0.608505</td>\n",
       "      <td>-0.733612</td>\n",
       "      <td>-0.047852</td>\n",
       "      <td>-0.606171</td>\n",
       "      <td>-0.747757</td>\n",
       "      <td>-0.047119</td>\n",
       "      <td>-0.605011</td>\n",
       "      <td>-0.768005</td>\n",
       "      <td>-0.059433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041275</td>\n",
       "      <td>-0.605270</td>\n",
       "      <td>-0.791214</td>\n",
       "      <td>-0.042770</td>\n",
       "      <td>-0.605270</td>\n",
       "      <td>-0.793640</td>\n",
       "      <td>-0.043015</td>\n",
       "      <td>-0.605026</td>\n",
       "      <td>-0.792908</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80220 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.146046  0.802520  0.586626 -0.031261  0.766375  0.666243 -0.042495   \n",
       "1      0.000000  0.771259  0.675035  0.007327  0.811312  0.539247 -0.040541   \n",
       "2      0.023446  0.778098  0.655009  0.005861  0.768329  0.650613  0.011723   \n",
       "3     -0.003419  0.760025  0.643775 -0.005861  0.762956  0.662336 -0.003908   \n",
       "4      0.033214  0.625702  0.784448  0.035168  0.613491  0.789820  0.040053   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "80215 -0.043182 -0.609863 -0.767822 -0.044266 -0.609787 -0.778061 -0.041946   \n",
       "80216 -0.040771 -0.604309 -0.789261 -0.042007 -0.603592 -0.789734 -0.040344   \n",
       "80217 -0.039551 -0.605042 -0.789276 -0.039154 -0.607315 -0.777832 -0.040588   \n",
       "80218 -0.047516 -0.608307 -0.784119 -0.048035 -0.610962 -0.787308 -0.047577   \n",
       "80219 -0.051819 -0.608505 -0.733612 -0.047852 -0.606171 -0.747757 -0.047119   \n",
       "\n",
       "              7         8         9  ...       375       376       377  \\\n",
       "0      0.769794  0.647682 -0.025888  ...  0.005373  0.760025  0.651101   \n",
       "1      0.866995  0.520686 -0.059102  ... -0.042983  0.786890  0.623260   \n",
       "2      0.773702  0.648659  0.007327  ...  0.013677  0.621306  0.786401   \n",
       "3      0.759537  0.664778  0.000488  ...  0.042495  0.703365  0.703365   \n",
       "4      0.615445  0.783959  0.034680  ...  0.022469  0.681874  0.725345   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "80215 -0.607758 -0.787079 -0.039627  ... -0.043823 -0.607330 -0.780472   \n",
       "80216 -0.602325 -0.793396 -0.039627  ... -0.036362 -0.609390 -0.790054   \n",
       "80217 -0.605408 -0.774155 -0.043030  ... -0.050720 -0.609039 -0.787018   \n",
       "80218 -0.609482 -0.789734 -0.045151  ... -0.041763 -0.605042 -0.790970   \n",
       "80219 -0.605011 -0.768005 -0.059433  ... -0.041275 -0.605270 -0.791214   \n",
       "\n",
       "            378       379       380       381       382       383     user  \n",
       "0     -0.006350  0.763444  0.663312 -0.015142  0.773213  0.661847  06mdn3c  \n",
       "1     -0.004884  0.777609  0.630098 -0.108435  0.807893  0.972500  06mdn3c  \n",
       "2      0.014653  0.626679  0.783959  0.021492  0.619841  0.780052  06mdn3c  \n",
       "3      0.057637  0.652078  0.743418  0.050310  0.635471  0.778586  06mdn3c  \n",
       "4      0.020026  0.682362  0.750256  0.020026  0.675524  0.740976  06mdn3c  \n",
       "...         ...       ...       ...       ...       ...       ...      ...  \n",
       "80215 -0.045563 -0.607315 -0.783386 -0.040222 -0.604111 -0.783401  x8rbf3x  \n",
       "80216 -0.037643 -0.607178 -0.793930 -0.040359 -0.603043 -0.794861  x8rbf3x  \n",
       "80217 -0.053665 -0.606857 -0.786743 -0.051178 -0.609070 -0.783844  x8rbf3x  \n",
       "80218 -0.043243 -0.605270 -0.792664 -0.045898 -0.604340 -0.789459  x8rbf3x  \n",
       "80219 -0.042770 -0.605270 -0.793640 -0.043015 -0.605026 -0.792908  x8rbf3x  \n",
       "\n",
       "[80220 rows x 385 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20e10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_rows(train_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d78657",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    userids = create_userids( df )\n",
    "    nbclasses = len(userids)    \n",
    "    array = df.values\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures -1 \n",
    "    X = array[:,0:nfeatures]\n",
    "    y = array[:,-1]\n",
    "    \n",
    "    print(y)\n",
    "    \n",
    "    enc = LabelEncoder()\n",
    "    y = enc.fit_transform(y.reshape(-1,1))\n",
    "    X = X.reshape(-1, 128, 3)\n",
    "    \n",
    "    print(y)\n",
    "\n",
    "#     def batch_format_fn(element):\n",
    "#         \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    "#         return collections.OrderedDict(\n",
    "#             x=tf.reshape(element['pixels'], [-1, 784]),\n",
    "#             y=tf.reshape(element['label'], [-1, 1]))\n",
    "\n",
    "#     return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b6a2ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['06mdn3c' '06mdn3c' '06mdn3c' ... 'x8rbf3x' 'x8rbf3x' 'x8rbf3x']\n",
      "[ 0  0  0 ... 29 29 29]\n"
     ]
    }
   ],
   "source": [
    "preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1860e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "userids = create_userids( df )\n",
    "nbclasses = len(userids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "104a2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate a model\n",
    "def evaluate_model(df, input_shape, num_filters = 128):\n",
    "    RANDOM_STATE = 11235\n",
    "    \n",
    "    userids = create_userids( df )\n",
    "    nbclasses = len(userids)\n",
    "    print(nbclasses)\n",
    "    array = df.values\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures -1 \n",
    "    X = array[:,0:nfeatures]\n",
    "    y = array[:,-1]\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y.reshape(-1,1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "    X = X.reshape(-1, 128, 3)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(X_val.shape)\n",
    "    \n",
    "    mini_batch_size = int(min(X_train.shape[0]/10, 16))   \n",
    "    \n",
    "    # set epochs and batch_size to 1 each due to its purpose solely as example and limiting resource\n",
    "    # set verbose to 1 to see training progress\n",
    "    verbose, epochs, batch_size = 1, 1, 1\n",
    "    EPOCHS = 100\n",
    "    \n",
    "    input_layer = keras.layers.Input(input_shape) \n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same')(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(num_filters, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    output_layer = keras.layers.Dense(nbclasses, activation='softmax')(gap_layer)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    learning_rate = 0.0001\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, \n",
    "                                                  min_lr=learning_rate)\n",
    "    \n",
    "    cb = [reduce_lr]\n",
    "    \n",
    "    # Set precision and recall to calculate F1 score\n",
    "    precision = tf.keras.metrics.Precision(name='precision')\n",
    "    recall = tf.keras.metrics.Recall(name='recall')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['accuracy', precision, recall]) \n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    X_train = np.asarray(X_train).astype(np.float32)\n",
    "    X_val = np.asarray(X_val).astype(np.float32)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    BATCH_SIZE = mini_batch_size\n",
    "    SHUFFLE_BUFFER_SIZE = 100\n",
    "    \n",
    "    train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    hist = model.fit(train_ds, \n",
    "                      epochs=EPOCHS,\n",
    "                      verbose=True, \n",
    "                      validation_data=val_ds, \n",
    "                      callbacks=cb)\n",
    "    \n",
    "    hist_df = pd.DataFrame(hist.history) \n",
    "    \n",
    "    print(hist_df)\n",
    "    \n",
    "    # get evaluation metrics\n",
    "    # categorical_accuracy = hist.history['categorical_accuracy'][epochs-1]\n",
    "    # accuracy = hist.history['accuracy'][epochs-1]\n",
    "    precision = hist.history['precision'][epochs-1]\n",
    "    recall = hist.history['recall'][epochs-1]\n",
    "    \n",
    "    X_test = np.asarray(X_test).astype(np.float32)    \n",
    "    y_true = np.argmax( y_test, axis=1)\n",
    "    y_pred = np.argmax( model.predict(X_test), axis=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred) \n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores, f1):\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    m2, s2 = np.mean(f1), np.std(f1)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f), F1 score: %.3f%% (+/-%.3f)' % (m, s, m2, s2))\n",
    "    \n",
    "# run an experiment\n",
    "def run_experiment():\n",
    "    accuracy, precision, recall = evaluate_model(df, (128, 3))\n",
    "    print(accuracy)\n",
    "    accuracy = accuracy * 100.0\n",
    "    f1_score = (2.0*((precision * recall)/(precision + recall))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2630fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cef2fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "\n",
    "    BATCH_SIZE = 30\n",
    "    SHUFFLE_BUFFER_SIZE = 100\n",
    "    \n",
    "    return dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cc2496d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_keras_model_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SOUTHS~1\\AppData\\Local\\Temp/ipykernel_2956/4209591276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_keras_model_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_keras_model_seq' is not defined"
     ]
    }
   ],
   "source": [
    "keras_model = create_keras_model_seq((128,3))\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8573d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_client_dataset(df):\n",
    "    RANDOM_STATE = 11235\n",
    "    userids = create_userids( df )\n",
    "    nbclasses = len(userids)\n",
    "    array = df.values\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures -1 \n",
    "    X = array[:,0:nfeatures]\n",
    "    y = array[:,-1]\n",
    "        \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y.reshape(-1,1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "    X = X.reshape(-1, 128, 3)\n",
    "    \n",
    "    print(y.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    \n",
    "    mini_batch_size = int(min(X_train.shape[0]/10, 16))   \n",
    "\n",
    "    X_train = np.asarray(X_train).astype(np.float32)\n",
    "    X_val = np.asarray(X_val).astype(np.float32)\n",
    "    \n",
    "    return create_clients(X_train, y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b01aaa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80220, 30)\n"
     ]
    }
   ],
   "source": [
    "clients = load_client_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08d42a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model_seq(input_shape, num_filters = 128):\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv1D(num_filters, kernel_size=3,padding='same', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(nbclasses, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b488ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_keras_model_seq((128,3))\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=clients_batched[0].element_spec,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc'),\\\n",
    "               tf.keras.metrics.Precision(name='pr'),\\\n",
    "               tf.keras.metrics.Recall(name='rc')\\\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f068a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86e41887",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_batched = list(clients_batched.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72951336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 128, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 30), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_batched[0].element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8e7203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration #1 | Accuracy: 45.870%, F1 score: 31.198%\n",
      "> Iteration #2 | Accuracy: 50.424%, F1 score: 40.869%\n",
      "> Iteration #3 | Accuracy: 52.146%, F1 score: 44.170%\n",
      "> Iteration #4 | Accuracy: 53.605%, F1 score: 46.445%\n",
      "> Iteration #5 | Accuracy: 54.667%, F1 score: 47.983%\n",
      "> Iteration #6 | Accuracy: 55.159%, F1 score: 49.686%\n",
      "> Iteration #7 | Accuracy: 55.816%, F1 score: 50.176%\n",
      "> Iteration #8 | Accuracy: 56.194%, F1 score: 50.849%\n",
      "> Iteration #9 | Accuracy: 56.971%, F1 score: 52.164%\n",
      "> Iteration #10 | Accuracy: 57.550%, F1 score: 53.070%\n",
      "> Iteration #11 | Accuracy: 57.769%, F1 score: 53.310%\n",
      "> Iteration #12 | Accuracy: 58.194%, F1 score: 53.719%\n",
      "> Iteration #13 | Accuracy: 59.125%, F1 score: 54.894%\n",
      "> Iteration #14 | Accuracy: 59.404%, F1 score: 55.431%\n",
      "> Iteration #15 | Accuracy: 59.904%, F1 score: 55.865%\n",
      "> Iteration #16 | Accuracy: 60.272%, F1 score: 56.518%\n",
      "> Iteration #17 | Accuracy: 60.740%, F1 score: 57.235%\n",
      "> Iteration #18 | Accuracy: 60.920%, F1 score: 57.512%\n",
      "> Iteration #19 | Accuracy: 61.340%, F1 score: 58.050%\n",
      "> Iteration #20 | Accuracy: 61.587%, F1 score: 58.596%\n",
      "> Iteration #21 | Accuracy: 62.011%, F1 score: 58.935%\n",
      "> Iteration #22 | Accuracy: 62.261%, F1 score: 59.317%\n",
      "> Iteration #23 | Accuracy: 62.464%, F1 score: 59.744%\n",
      "> Iteration #24 | Accuracy: 62.716%, F1 score: 60.092%\n",
      "> Iteration #25 | Accuracy: 62.873%, F1 score: 60.428%\n",
      "> Iteration #26 | Accuracy: 63.285%, F1 score: 60.700%\n",
      "> Iteration #27 | Accuracy: 63.673%, F1 score: 61.350%\n",
      "> Iteration #28 | Accuracy: 63.852%, F1 score: 61.488%\n",
      "> Iteration #29 | Accuracy: 64.112%, F1 score: 61.794%\n",
      "> Iteration #30 | Accuracy: 64.079%, F1 score: 62.137%\n",
      "> Iteration #31 | Accuracy: 64.529%, F1 score: 62.494%\n",
      "> Iteration #32 | Accuracy: 64.827%, F1 score: 62.613%\n",
      "> Iteration #33 | Accuracy: 64.959%, F1 score: 63.151%\n",
      "> Iteration #34 | Accuracy: 65.101%, F1 score: 63.463%\n",
      "> Iteration #35 | Accuracy: 65.396%, F1 score: 63.638%\n",
      "> Iteration #36 | Accuracy: 65.410%, F1 score: 63.710%\n",
      "> Iteration #37 | Accuracy: 65.924%, F1 score: 64.366%\n",
      "> Iteration #38 | Accuracy: 65.799%, F1 score: 64.101%\n",
      "> Iteration #39 | Accuracy: 66.073%, F1 score: 64.628%\n",
      "> Iteration #40 | Accuracy: 66.509%, F1 score: 64.937%\n",
      "> Iteration #41 | Accuracy: 66.476%, F1 score: 65.196%\n",
      "> Iteration #42 | Accuracy: 66.711%, F1 score: 65.324%\n",
      "> Iteration #43 | Accuracy: 66.809%, F1 score: 65.673%\n",
      "> Iteration #44 | Accuracy: 67.341%, F1 score: 66.010%\n",
      "> Iteration #45 | Accuracy: 67.041%, F1 score: 65.954%\n",
      "> Iteration #46 | Accuracy: 67.546%, F1 score: 66.249%\n",
      "> Iteration #47 | Accuracy: 67.577%, F1 score: 66.259%\n",
      "> Iteration #48 | Accuracy: 67.748%, F1 score: 66.626%\n",
      "> Iteration #49 | Accuracy: 67.706%, F1 score: 66.725%\n",
      "> Iteration #50 | Accuracy: 68.028%, F1 score: 66.984%\n",
      "> Iteration #51 | Accuracy: 67.978%, F1 score: 66.804%\n",
      "> Iteration #52 | Accuracy: 68.338%, F1 score: 67.140%\n",
      "> Iteration #53 | Accuracy: 68.398%, F1 score: 67.423%\n",
      "> Iteration #54 | Accuracy: 68.433%, F1 score: 67.432%\n",
      "> Iteration #55 | Accuracy: 68.697%, F1 score: 67.687%\n",
      "> Iteration #56 | Accuracy: 68.807%, F1 score: 67.865%\n",
      "> Iteration #57 | Accuracy: 69.090%, F1 score: 68.010%\n",
      "> Iteration #58 | Accuracy: 69.005%, F1 score: 67.849%\n",
      "> Iteration #59 | Accuracy: 68.976%, F1 score: 68.277%\n",
      "> Iteration #60 | Accuracy: 69.458%, F1 score: 68.451%\n",
      "> Iteration #61 | Accuracy: 69.543%, F1 score: 68.566%\n",
      "> Iteration #62 | Accuracy: 69.620%, F1 score: 68.666%\n",
      "> Iteration #63 | Accuracy: 69.699%, F1 score: 68.846%\n",
      "> Iteration #64 | Accuracy: 69.715%, F1 score: 68.693%\n",
      "> Iteration #65 | Accuracy: 69.963%, F1 score: 69.326%\n",
      "> Iteration #66 | Accuracy: 69.921%, F1 score: 69.217%\n",
      "> Iteration #67 | Accuracy: 70.270%, F1 score: 69.299%\n",
      "> Iteration #68 | Accuracy: 70.152%, F1 score: 69.480%\n",
      "> Iteration #69 | Accuracy: 70.330%, F1 score: 69.695%\n",
      "> Iteration #70 | Accuracy: 70.563%, F1 score: 69.668%\n",
      "> Iteration #71 | Accuracy: 70.671%, F1 score: 69.965%\n",
      "> Iteration #72 | Accuracy: 70.700%, F1 score: 69.823%\n",
      "> Iteration #73 | Accuracy: 71.004%, F1 score: 70.089%\n",
      "> Iteration #74 | Accuracy: 71.388%, F1 score: 70.392%\n",
      "> Iteration #75 | Accuracy: 71.230%, F1 score: 70.549%\n",
      "> Iteration #76 | Accuracy: 71.253%, F1 score: 70.380%\n",
      "> Iteration #77 | Accuracy: 71.619%, F1 score: 70.765%\n",
      "> Iteration #78 | Accuracy: 71.556%, F1 score: 70.799%\n",
      "> Iteration #79 | Accuracy: 71.538%, F1 score: 70.915%\n",
      "> Iteration #80 | Accuracy: 71.348%, F1 score: 70.629%\n",
      "> Iteration #81 | Accuracy: 71.550%, F1 score: 71.036%\n",
      "> Iteration #82 | Accuracy: 71.764%, F1 score: 71.136%\n",
      "> Iteration #83 | Accuracy: 71.843%, F1 score: 71.253%\n",
      "> Iteration #84 | Accuracy: 71.833%, F1 score: 71.009%\n",
      "> Iteration #85 | Accuracy: 72.244%, F1 score: 71.617%\n",
      "> Iteration #86 | Accuracy: 72.292%, F1 score: 71.630%\n",
      "> Iteration #87 | Accuracy: 72.084%, F1 score: 71.501%\n",
      "> Iteration #88 | Accuracy: 72.069%, F1 score: 71.586%\n",
      "> Iteration #89 | Accuracy: 72.414%, F1 score: 71.779%\n",
      "> Iteration #90 | Accuracy: 72.404%, F1 score: 71.826%\n",
      "> Iteration #91 | Accuracy: 72.256%, F1 score: 71.722%\n",
      "> Iteration #92 | Accuracy: 72.730%, F1 score: 72.209%\n",
      "> Iteration #93 | Accuracy: 72.747%, F1 score: 72.067%\n",
      "> Iteration #94 | Accuracy: 72.780%, F1 score: 72.134%\n",
      "> Iteration #95 | Accuracy: 72.923%, F1 score: 72.506%\n",
      "> Iteration #96 | Accuracy: 72.905%, F1 score: 72.440%\n",
      "> Iteration #97 | Accuracy: 72.738%, F1 score: 72.171%\n",
      "> Iteration #98 | Accuracy: 73.169%, F1 score: 72.509%\n",
      "> Iteration #99 | Accuracy: 73.281%, F1 score: 72.496%\n",
      "> Iteration #100 | Accuracy: 73.162%, F1 score: 72.649%\n"
     ]
    }
   ],
   "source": [
    "trainer = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.0001))\n",
    "\n",
    "# Init\n",
    "state = trainer.initialize()\n",
    "\n",
    "# Simulate \"batch\" learning\n",
    "for i in range(100):\n",
    "    state, metrics = trainer.next(state, clients_batched)\n",
    "    print('> Iteration #%d | Accuracy: %.3f%%, F1 score: %.3f%%' % ( \\\n",
    "            i+1, \\\n",
    "            metrics['train']['acc'] * 100.0, \\\n",
    "            2.0*((metrics['train']['pr'] * metrics['train']['rc'])/(metrics['train']['pr'] + metrics['train']['rc'])) * 100.0 \\\n",
    "         )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfb17f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(data_list, label_list, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(data_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fcn(input_shape, nb_classes, file_path, num_filters = 128):\n",
    "    input_layer = keras.layers.Input(input_shape) \n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same')(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(num_filters, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['categorical_accuracy'])\n",
    "    learning_rate = 0.0001\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, \n",
    "                                                  min_lr=learning_rate)\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cdabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd510464",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_fcn((128, 3), nbclasses, \"foo.h5\")\n",
    "comms_round = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:    \n",
    "        local_model = build_fcn((128, 3), nbclasses, \"foo.h5\")\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8257b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
